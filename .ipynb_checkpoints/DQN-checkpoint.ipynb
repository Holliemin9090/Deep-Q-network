{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    #\n",
    "    # Initializes attributes and constructs CNN model and target_model\n",
    "    #\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.9            # Discount rate\n",
    "        self.epsilon = 1.0          # Exploration rate\n",
    "        self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
    "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "        self.update_rate = 1000     # Number of steps until updating the target network\n",
    "        \n",
    "        # Construct DQN models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "\n",
    "    #\n",
    "    # Constructs CNN\n",
    "    #\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Conv Layers\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # FC Layers\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    #\n",
    "    # Stores experience in replay memory\n",
    "    #\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #\n",
    "    # Chooses action based on epsilon-greedy policy\n",
    "    #\n",
    "    def act(self, state):\n",
    "        # Random exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(act_values[0])  # Returns action using policy\n",
    "\n",
    "    #\n",
    "    # Trains the model using randomly selected experiences in the replay memory\n",
    "    #\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            # Construct the target vector as follows:\n",
    "            # 1. Use the current model to output the Q-value predictions\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            # 2. Rewrite the chosen action value with the computed target\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # 3. Use vectors in the objective computation\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    #\n",
    "    # Sets the target model parameters to the current model parameters\n",
    "    #\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "    #\n",
    "    # Loads a saved model\n",
    "    #\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    #\n",
    "    # Saves parameters of a trained model\n",
    "    #\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpful preprocessing taken from github.com/ageron/tiny-dqn\n",
    "def process_frame(frame):\n",
    "    mspacman_color = np.array([210, 164, 74]).mean()\n",
    "    img = frame[1:176:2, ::2]    # Crop and downsize\n",
    "    img = img.mean(axis=2)       # Convert to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
    "    img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
    "    \n",
    "    return np.expand_dims(img.reshape(88, 80, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_images(images, blend):\n",
    "    avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
    "\n",
    "    for image in images:\n",
    "        avg_image += image\n",
    "        \n",
    "    if len(images) < blend:\n",
    "        return avg_image / len(images)\n",
    "    else:\n",
    "        return avg_image / blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 22, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7040)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3604992   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 3,681,449\n",
      "Trainable params: 3,681,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "state_size = (88, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "agent = DQN_Agent(state_size, action_size)\n",
    "\n",
    "episodes = 500\n",
    "batch_size = 8\n",
    "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/500, game score: 620.0, reward: -208.0, avg reward: 620.0, time: 827, total time: 828\n",
      "episode: 2/500, game score: 560.0, reward: -370.0, avg reward: 590.0, time: 929, total time: 1758\n",
      "episode: 3/500, game score: 870.0, reward: -160.0, avg reward: 683.333333333, time: 1029, total time: 2788\n",
      "episode: 4/500, game score: 420.0, reward: -168.0, avg reward: 617.5, time: 587, total time: 3376\n",
      "episode: 5/500, game score: 540.0, reward: -177.0, avg reward: 602.0, time: 716, total time: 4093\n",
      "episode: 6/500, game score: 380.0, reward: -122.0, avg reward: 565.0, time: 501, total time: 4595\n",
      "episode: 7/500, game score: 330.0, reward: -102.0, avg reward: 531.428571429, time: 431, total time: 5027\n",
      "episode: 8/500, game score: 420.0, reward: -10.0, avg reward: 517.5, time: 429, total time: 5457\n",
      "episode: 9/500, game score: 190.0, reward: -111.0, avg reward: 481.111111111, time: 300, total time: 5758\n",
      "episode: 10/500, game score: 580.0, reward: -115.0, avg reward: 491.0, time: 694, total time: 6453\n",
      "episode: 11/500, game score: 480.0, reward: -75.0, avg reward: 490.0, time: 554, total time: 7008\n",
      "episode: 12/500, game score: 690.0, reward: -36.0, avg reward: 506.666666667, time: 725, total time: 7734\n",
      "episode: 13/500, game score: 250.0, reward: -103.0, avg reward: 486.923076923, time: 352, total time: 8087\n",
      "episode: 14/500, game score: 310.0, reward: -70.0, avg reward: 474.285714286, time: 379, total time: 8467\n",
      "episode: 15/500, game score: 420.0, reward: -47.0, avg reward: 470.666666667, time: 466, total time: 8934\n",
      "episode: 16/500, game score: 670.0, reward: 54.0, avg reward: 483.125, time: 615, total time: 9550\n",
      "episode: 17/500, game score: 850.0, reward: -236.0, avg reward: 504.705882353, time: 1085, total time: 10636\n",
      "episode: 18/500, game score: 1020.0, reward: 128.0, avg reward: 533.333333333, time: 891, total time: 11528\n",
      "episode: 19/500, game score: 480.0, reward: -263.0, avg reward: 530.526315789, time: 742, total time: 12271\n",
      "episode: 20/500, game score: 650.0, reward: -77.0, avg reward: 536.5, time: 726, total time: 12998\n",
      "episode: 21/500, game score: 780.0, reward: 113.0, avg reward: 548.095238095, time: 666, total time: 13665\n",
      "episode: 22/500, game score: 490.0, reward: -69.0, avg reward: 545.454545455, time: 558, total time: 14224\n",
      "episode: 23/500, game score: 340.0, reward: -46.0, avg reward: 536.52173913, time: 385, total time: 14610\n",
      "episode: 24/500, game score: 940.0, reward: 106.0, avg reward: 553.333333333, time: 833, total time: 15444\n",
      "episode: 25/500, game score: 590.0, reward: 59.0, avg reward: 554.8, time: 530, total time: 15975\n",
      "episode: 26/500, game score: 1360.0, reward: 302.0, avg reward: 585.769230769, time: 1057, total time: 17033\n",
      "episode: 27/500, game score: 1290.0, reward: 504.0, avg reward: 611.851851852, time: 785, total time: 17819\n",
      "episode: 28/500, game score: 210.0, reward: -543.0, avg reward: 597.5, time: 752, total time: 18572\n",
      "episode: 29/500, game score: 730.0, reward: -65.0, avg reward: 602.068965517, time: 794, total time: 19367\n",
      "episode: 30/500, game score: 440.0, reward: -249.0, avg reward: 596.666666667, time: 688, total time: 20056\n",
      "episode: 31/500, game score: 530.0, reward: -164.0, avg reward: 594.516129032, time: 693, total time: 20750\n",
      "episode: 32/500, game score: 290.0, reward: -153.0, avg reward: 585.0, time: 442, total time: 21193\n",
      "episode: 33/500, game score: 900.0, reward: -88.0, avg reward: 594.545454545, time: 987, total time: 22181\n",
      "episode: 34/500, game score: 640.0, reward: -287.0, avg reward: 595.882352941, time: 926, total time: 23108\n",
      "episode: 35/500, game score: 1070.0, reward: 311.0, avg reward: 609.428571429, time: 758, total time: 23867\n",
      "episode: 36/500, game score: 460.0, reward: -1.0, avg reward: 605.277777778, time: 460, total time: 24328\n",
      "episode: 37/500, game score: 370.0, reward: -208.0, avg reward: 598.918918919, time: 577, total time: 24906\n",
      "episode: 38/500, game score: 330.0, reward: -151.0, avg reward: 591.842105263, time: 480, total time: 25387\n",
      "episode: 39/500, game score: 520.0, reward: -280.0, avg reward: 590.0, time: 799, total time: 26187\n",
      "episode: 40/500, game score: 1070.0, reward: 260.0, avg reward: 602.0, time: 809, total time: 26997\n",
      "episode: 41/500, game score: 660.0, reward: -250.0, avg reward: 603.414634146, time: 909, total time: 27907\n",
      "episode: 42/500, game score: 910.0, reward: -80.0, avg reward: 610.714285714, time: 989, total time: 28897\n",
      "episode: 43/500, game score: 270.0, reward: -193.0, avg reward: 602.790697674, time: 462, total time: 29360\n",
      "episode: 44/500, game score: 200.0, reward: -298.0, avg reward: 593.636363636, time: 497, total time: 29858\n",
      "episode: 45/500, game score: 140.0, reward: -211.0, avg reward: 583.555555556, time: 350, total time: 30209\n",
      "episode: 46/500, game score: 480.0, reward: -301.0, avg reward: 581.304347826, time: 780, total time: 30990\n",
      "episode: 47/500, game score: 610.0, reward: -65.0, avg reward: 581.914893617, time: 674, total time: 31665\n",
      "episode: 48/500, game score: 330.0, reward: -151.0, avg reward: 576.666666667, time: 480, total time: 32146\n",
      "episode: 49/500, game score: 310.0, reward: -259.0, avg reward: 571.224489796, time: 568, total time: 32715\n",
      "episode: 50/500, game score: 700.0, reward: -1.0, avg reward: 573.8, time: 700, total time: 33416\n",
      "episode: 51/500, game score: 330.0, reward: -88.0, avg reward: 569.019607843, time: 417, total time: 33834\n",
      "episode: 52/500, game score: 360.0, reward: -179.0, avg reward: 565.0, time: 538, total time: 34373\n",
      "episode: 53/500, game score: 470.0, reward: -225.0, avg reward: 563.20754717, time: 694, total time: 35068\n",
      "episode: 54/500, game score: 800.0, reward: -135.0, avg reward: 567.592592593, time: 934, total time: 36003\n",
      "episode: 55/500, game score: 340.0, reward: -29.0, avg reward: 563.454545455, time: 368, total time: 36372\n",
      "episode: 56/500, game score: 690.0, reward: -163.0, avg reward: 565.714285714, time: 852, total time: 37225\n",
      "episode: 57/500, game score: 500.0, reward: -3.0, avg reward: 564.561403509, time: 502, total time: 37728\n",
      "episode: 58/500, game score: 1050.0, reward: 424.0, avg reward: 572.931034483, time: 625, total time: 38354\n",
      "episode: 59/500, game score: 660.0, reward: -8.0, avg reward: 574.406779661, time: 667, total time: 39022\n",
      "episode: 60/500, game score: 1400.0, reward: 418.0, avg reward: 588.166666667, time: 981, total time: 40004\n",
      "episode: 61/500, game score: 940.0, reward: 236.0, avg reward: 593.93442623, time: 703, total time: 40708\n",
      "episode: 62/500, game score: 820.0, reward: 52.0, avg reward: 597.580645161, time: 767, total time: 41476\n",
      "episode: 63/500, game score: 670.0, reward: -155.0, avg reward: 598.73015873, time: 824, total time: 42301\n",
      "episode: 64/500, game score: 340.0, reward: -290.0, avg reward: 594.6875, time: 629, total time: 42931\n",
      "episode: 65/500, game score: 490.0, reward: -195.0, avg reward: 593.076923077, time: 684, total time: 43616\n",
      "episode: 66/500, game score: 430.0, reward: -81.0, avg reward: 590.606060606, time: 510, total time: 44127\n",
      "episode: 67/500, game score: 1200.0, reward: 172.0, avg reward: 599.701492537, time: 1027, total time: 45155\n",
      "episode: 68/500, game score: 270.0, reward: -278.0, avg reward: 594.852941176, time: 547, total time: 45703\n",
      "episode: 69/500, game score: 960.0, reward: -334.0, avg reward: 600.144927536, time: 1293, total time: 46997\n",
      "episode: 70/500, game score: 480.0, reward: -234.0, avg reward: 598.428571429, time: 713, total time: 47711\n",
      "episode: 71/500, game score: 390.0, reward: -158.0, avg reward: 595.492957746, time: 547, total time: 48259\n",
      "episode: 72/500, game score: 610.0, reward: -90.0, avg reward: 595.694444444, time: 699, total time: 48959\n",
      "episode: 73/500, game score: 1080.0, reward: -111.0, avg reward: 602.328767123, time: 1190, total time: 50150\n",
      "episode: 74/500, game score: 1170.0, reward: 373.0, avg reward: 610.0, time: 796, total time: 50947\n",
      "episode: 75/500, game score: 110.0, reward: -213.0, avg reward: 603.333333333, time: 322, total time: 51270\n",
      "episode: 76/500, game score: 550.0, reward: -153.0, avg reward: 602.631578947, time: 702, total time: 51973\n",
      "episode: 77/500, game score: 770.0, reward: -58.0, avg reward: 604.805194805, time: 827, total time: 52801\n",
      "episode: 78/500, game score: 330.0, reward: -264.0, avg reward: 601.282051282, time: 593, total time: 53395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 79/500, game score: 330.0, reward: -59.0, avg reward: 597.848101266, time: 388, total time: 53784\n",
      "episode: 80/500, game score: 1110.0, reward: -296.0, avg reward: 604.25, time: 1405, total time: 55190\n",
      "episode: 81/500, game score: 620.0, reward: -67.0, avg reward: 604.444444444, time: 686, total time: 55877\n",
      "episode: 82/500, game score: 480.0, reward: -132.0, avg reward: 602.926829268, time: 611, total time: 56489\n",
      "episode: 83/500, game score: 300.0, reward: -192.0, avg reward: 599.277108434, time: 491, total time: 56981\n",
      "episode: 84/500, game score: 300.0, reward: -179.0, avg reward: 595.714285714, time: 478, total time: 57460\n",
      "episode: 85/500, game score: 1610.0, reward: 773.0, avg reward: 607.647058824, time: 836, total time: 58297\n",
      "episode: 86/500, game score: 180.0, reward: -226.0, avg reward: 602.674418605, time: 405, total time: 58703\n",
      "episode: 87/500, game score: 370.0, reward: -451.0, avg reward: 600.0, time: 820, total time: 59524\n",
      "episode: 88/500, game score: 320.0, reward: -128.0, avg reward: 596.818181818, time: 447, total time: 59972\n",
      "episode: 89/500, game score: 890.0, reward: 14.0, avg reward: 600.112359551, time: 875, total time: 60848\n",
      "episode: 90/500, game score: 350.0, reward: -274.0, avg reward: 597.333333333, time: 623, total time: 61472\n",
      "episode: 91/500, game score: 1220.0, reward: 265.0, avg reward: 604.175824176, time: 954, total time: 62427\n",
      "episode: 92/500, game score: 1120.0, reward: -28.0, avg reward: 609.782608696, time: 1147, total time: 63575\n",
      "episode: 93/500, game score: 230.0, reward: -210.0, avg reward: 605.698924731, time: 439, total time: 64015\n",
      "episode: 94/500, game score: 1280.0, reward: 240.0, avg reward: 612.872340426, time: 1039, total time: 65055\n",
      "episode: 95/500, game score: 190.0, reward: -228.0, avg reward: 608.421052632, time: 417, total time: 65473\n",
      "episode: 96/500, game score: 150.0, reward: -255.0, avg reward: 603.645833333, time: 404, total time: 65878\n",
      "episode: 97/500, game score: 540.0, reward: -44.0, avg reward: 602.989690722, time: 583, total time: 66462\n",
      "episode: 98/500, game score: 1010.0, reward: 182.0, avg reward: 607.142857143, time: 827, total time: 67290\n",
      "episode: 99/500, game score: 350.0, reward: -67.0, avg reward: 604.545454545, time: 416, total time: 67707\n",
      "episode: 100/500, game score: 450.0, reward: -127.0, avg reward: 603.0, time: 576, total time: 68284\n",
      "episode: 101/500, game score: 510.0, reward: -96.0, avg reward: 602.079207921, time: 605, total time: 68890\n",
      "episode: 102/500, game score: 380.0, reward: -319.0, avg reward: 599.901960784, time: 698, total time: 69589\n",
      "episode: 103/500, game score: 820.0, reward: 17.0, avg reward: 602.038834951, time: 802, total time: 70392\n",
      "episode: 104/500, game score: 880.0, reward: 295.0, avg reward: 604.711538462, time: 584, total time: 70977\n",
      "episode: 105/500, game score: 290.0, reward: -329.0, avg reward: 601.714285714, time: 618, total time: 71596\n",
      "episode: 106/500, game score: 630.0, reward: 58.0, avg reward: 601.981132075, time: 571, total time: 72168\n",
      "episode: 107/500, game score: 850.0, reward: 136.0, avg reward: 604.299065421, time: 713, total time: 72882\n",
      "episode: 108/500, game score: 280.0, reward: -127.0, avg reward: 601.296296296, time: 406, total time: 73289\n",
      "episode: 109/500, game score: 400.0, reward: -126.0, avg reward: 599.449541284, time: 525, total time: 73815\n",
      "episode: 110/500, game score: 1260.0, reward: 357.0, avg reward: 605.454545455, time: 902, total time: 74718\n",
      "episode: 111/500, game score: 1550.0, reward: 630.0, avg reward: 613.963963964, time: 919, total time: 75638\n",
      "episode: 112/500, game score: 420.0, reward: -120.0, avg reward: 612.232142857, time: 539, total time: 76178\n",
      "episode: 113/500, game score: 300.0, reward: -167.0, avg reward: 609.469026549, time: 466, total time: 76645\n",
      "episode: 114/500, game score: 1060.0, reward: 330.0, avg reward: 613.421052632, time: 729, total time: 77375\n",
      "episode: 115/500, game score: 400.0, reward: -149.0, avg reward: 611.565217391, time: 548, total time: 77924\n",
      "episode: 116/500, game score: 370.0, reward: -243.0, avg reward: 609.482758621, time: 612, total time: 78537\n",
      "episode: 117/500, game score: 560.0, reward: -82.0, avg reward: 609.05982906, time: 641, total time: 79179\n",
      "episode: 118/500, game score: 1240.0, reward: 190.0, avg reward: 614.406779661, time: 1049, total time: 80229\n",
      "episode: 119/500, game score: 310.0, reward: -208.0, avg reward: 611.848739496, time: 517, total time: 80747\n",
      "episode: 120/500, game score: 1370.0, reward: 271.0, avg reward: 618.166666667, time: 1098, total time: 81846\n",
      "episode: 121/500, game score: 1150.0, reward: -118.0, avg reward: 622.561983471, time: 1267, total time: 83114\n",
      "episode: 122/500, game score: 330.0, reward: -115.0, avg reward: 620.163934426, time: 444, total time: 83559\n",
      "episode: 123/500, game score: 210.0, reward: -267.0, avg reward: 616.829268293, time: 476, total time: 84036\n",
      "episode: 124/500, game score: 690.0, reward: -175.0, avg reward: 617.419354839, time: 864, total time: 84901\n",
      "episode: 125/500, game score: 1920.0, reward: 1094.0, avg reward: 627.84, time: 825, total time: 85727\n",
      "episode: 126/500, game score: 460.0, reward: -114.0, avg reward: 626.507936508, time: 573, total time: 86301\n",
      "episode: 127/500, game score: 500.0, reward: -189.0, avg reward: 625.511811024, time: 688, total time: 86990\n",
      "episode: 128/500, game score: 410.0, reward: -357.0, avg reward: 623.828125, time: 766, total time: 87757\n",
      "episode: 129/500, game score: 370.0, reward: -189.0, avg reward: 621.860465116, time: 558, total time: 88316\n",
      "episode: 130/500, game score: 180.0, reward: -179.0, avg reward: 618.461538462, time: 358, total time: 88675\n",
      "episode: 131/500, game score: 980.0, reward: 334.0, avg reward: 621.221374046, time: 645, total time: 89321\n",
      "episode: 132/500, game score: 380.0, reward: -237.0, avg reward: 619.393939394, time: 616, total time: 89938\n",
      "episode: 133/500, game score: 790.0, reward: -19.0, avg reward: 620.676691729, time: 808, total time: 90747\n",
      "episode: 134/500, game score: 560.0, reward: -164.0, avg reward: 620.223880597, time: 723, total time: 91471\n",
      "episode: 135/500, game score: 110.0, reward: -218.0, avg reward: 616.444444444, time: 327, total time: 91799\n",
      "episode: 136/500, game score: 350.0, reward: -193.0, avg reward: 614.485294118, time: 542, total time: 92342\n",
      "episode: 137/500, game score: 580.0, reward: -182.0, avg reward: 614.233576642, time: 761, total time: 93104\n",
      "episode: 138/500, game score: 230.0, reward: -299.0, avg reward: 611.449275362, time: 528, total time: 93633\n",
      "episode: 139/500, game score: 360.0, reward: -136.0, avg reward: 609.64028777, time: 495, total time: 94129\n",
      "episode: 140/500, game score: 500.0, reward: -269.0, avg reward: 608.857142857, time: 768, total time: 94898\n",
      "episode: 141/500, game score: 200.0, reward: -162.0, avg reward: 605.957446809, time: 361, total time: 95260\n",
      "episode: 142/500, game score: 470.0, reward: -354.0, avg reward: 605.0, time: 823, total time: 96084\n",
      "episode: 143/500, game score: 2300.0, reward: 1111.0, avg reward: 616.853146853, time: 1188, total time: 97273\n",
      "episode: 144/500, game score: 680.0, reward: 51.0, avg reward: 617.291666667, time: 628, total time: 97902\n",
      "episode: 145/500, game score: 320.0, reward: -132.0, avg reward: 615.24137931, time: 451, total time: 98354\n",
      "episode: 146/500, game score: 1690.0, reward: 666.0, avg reward: 622.602739726, time: 1023, total time: 99378\n",
      "episode: 147/500, game score: 240.0, reward: -221.0, avg reward: 620.0, time: 460, total time: 99839\n",
      "episode: 148/500, game score: 260.0, reward: -287.0, avg reward: 617.567567568, time: 546, total time: 100386\n",
      "episode: 149/500, game score: 420.0, reward: -102.0, avg reward: 616.241610738, time: 521, total time: 100908\n",
      "episode: 150/500, game score: 210.0, reward: -303.0, avg reward: 613.533333333, time: 512, total time: 101421\n",
      "episode: 151/500, game score: 180.0, reward: -149.0, avg reward: 610.662251656, time: 328, total time: 101750\n",
      "episode: 152/500, game score: 420.0, reward: -82.0, avg reward: 609.407894737, time: 501, total time: 102252\n",
      "episode: 153/500, game score: 340.0, reward: -209.0, avg reward: 607.647058824, time: 548, total time: 102801\n",
      "episode: 154/500, game score: 270.0, reward: -158.0, avg reward: 605.454545455, time: 427, total time: 103229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 155/500, game score: 650.0, reward: -151.0, avg reward: 605.741935484, time: 800, total time: 104030\n",
      "episode: 156/500, game score: 540.0, reward: -150.0, avg reward: 605.320512821, time: 689, total time: 104720\n",
      "episode: 157/500, game score: 350.0, reward: -238.0, avg reward: 603.694267516, time: 587, total time: 105308\n",
      "episode: 158/500, game score: 970.0, reward: -261.0, avg reward: 606.012658228, time: 1230, total time: 106539\n",
      "episode: 159/500, game score: 450.0, reward: -86.0, avg reward: 605.031446541, time: 535, total time: 107075\n",
      "episode: 160/500, game score: 620.0, reward: -21.0, avg reward: 605.125, time: 640, total time: 107716\n",
      "episode: 161/500, game score: 390.0, reward: -355.0, avg reward: 603.788819876, time: 744, total time: 108461\n",
      "episode: 162/500, game score: 410.0, reward: -43.0, avg reward: 602.592592593, time: 452, total time: 108914\n",
      "episode: 163/500, game score: 1020.0, reward: 21.0, avg reward: 605.153374233, time: 998, total time: 109913\n",
      "episode: 164/500, game score: 990.0, reward: 187.0, avg reward: 607.5, time: 802, total time: 110716\n"
     ]
    }
   ],
   "source": [
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    game_score = 0\n",
    "    state = process_frame(env.reset())\n",
    "    images = deque(maxlen=blend)  # Array of images to be blended\n",
    "    images.append(state)\n",
    "    \n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    \n",
    "    for time in range(20000):\n",
    "        env.render()\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps we update the target network parameters\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        state = blend_images(images, blend)\n",
    "        \n",
    "        # Transition Dynamics\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        next_state = process_frame(next_state)\n",
    "        images.append(next_state)\n",
    "        next_state = blend_images(images, blend)\n",
    "        \n",
    "        # Store sequence in replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        game_score += reward\n",
    "        reward -= 1  # Punish behavior which does not accumulate reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))\n",
    "            \n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.save('models/5k-memory_500-games')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
